{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by importing some of the libraries we will need and set up our notebook session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Basic Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here goes the blueprint of our perceptron, encoded as a plain Python class, as well as a single auxiliary function. Below we will walk through the details of each code components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, nb_features=None):\n",
    "        self.weights = None\n",
    "        self.nb_features = nb_features # i.e. the nb of incoming connections from other neurons\n",
    "    \n",
    "    def set_weights(self, weights=None):\n",
    "        if weights:\n",
    "            self.weights = weights\n",
    "        else:\n",
    "            self.weights = np.ones(self.nb_features)\n",
    "    \n",
    "    def predict(self, feature_vectors, squash=False):\n",
    "        scores = []\n",
    "        for fv in feature_vectors:\n",
    "            s = 0.0\n",
    "            for i in range(self.nb_features):\n",
    "                s += self.weights[i] * fv[i]\n",
    "            scores.append(s)\n",
    "        \n",
    "        if squash:\n",
    "            scores = [sigmoid(v) for v in scores]\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by predicting house prices in a really naive way, i.e. we assume that all characteristics are equally important, each having a (positive) weight of 1. We characterize each house along 5 integer variables, namely:\n",
    "* the number of doors\n",
    "* surface (in square meters)\n",
    "* the number of bedrooms\n",
    "* the number of bathrooms\n",
    "* its age (in years)\n",
    "\n",
    "First, we initialize our perceptron, and we specify that we will use 5 input features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perceptron = Perceptron(nb_features=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set the weights of our perceptron; by default, our perceptron will assign an equal, positive weight of 1.0 to each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "perceptron.set_weights()\n",
    "print(perceptron.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a dummy data set of 5 houses, which are represented as a fixed length vector of five integers (the index of which corresponds to the bullet list above): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "houses = [[2, 3, 5, 1, 8],\n",
    "          [1, 2, 1, 3, 5],\n",
    "          [4, 4, 2, 1, 3],\n",
    "          [11, 6, 8, 9, 8],\n",
    "          [9, 10, 8, 1, 30]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this data set, the last houses have higher a number of doors, bathrooms etc., so that they would have to predict higher prices. We now feed this data set to the `predict()` function of our perceptron (cf. standard `sklearn` naming conventions), and we have it predict a the price for each house:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prices = perceptron.predict(houses)\n",
    "print(prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the see that for instance the pentultimate house has a relatively higher price. Of course, our feature weighting is now ridiculously naive. Clearly,\n",
    "* the number of bathrooms should matter relative more than the number of doors\n",
    "* the age of a house should negatively affect the total price\n",
    "\n",
    "We can now fix this by setting a more sensible weight vector, which should have five entries (one for each feature). Using these new weights, we should now obtain a more accurate estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = [0.5, # doors\n",
    "           0.9, # surface\n",
    "           0.8, # bedrooms\n",
    "           0.9, # bathrooms\n",
    "           -0.5] # age\n",
    "perceptron.set_weights(weights)\n",
    "print(perceptron.predict(houses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the price of a house is simple enough and comes down to calculating the **weighted sum** of the feature values. We now see, for instance, that the final house will be valued much less than the penultimate house in the list, based on the assumption that, while is it much larger, it is also is much older and will therefore require much more investments. Note that the prices predicted are not in an actual currency and vary wildly. In many applications, it is very common to **normalize** the predictions of a perceptron and 'squash' them to a range between 0 and 1. Historically, the **sigmoid** function has often been for this. By setting `squash = True`, we can implement this behaviour in our perceptron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(perceptron.predict(houses, squash=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see that we obtain predictions which are neatly between 0 and 1. In the context of deep learning, functions such as the sigmoid function are often called **activation** functions, because they control how strongly a particular neuron will be activated. In our perceptron, we have a single output neuron, the activation of which is controlled via a sigmoid activation. We call such an activation **element-wise**, is they are applied to each element in a list individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speeding up our perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron which we created will be slow, because, right now, it only relies on traditional, unoptimized Python code. Inspect for instance the code block in the `predict()` function above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The many `for`-loops in this code will make it extremely slow to run. In Deep Learning (and in so many other fields), such code will be too slow to be usable in practice: it is much more common to used **vectorized routines** from specialized libraries, such as `numpy`, which can easily replace our naive `for`-loops. To be able to vectorize our code more efficiently, we will have to convert our Python lists to `numpy` arrays, which essentially are matrix objects, that have a **`shape`** attribute. As will become clear below, the `shape` is an extremely important property of `numpy` arrays, and you will want to keep track of it frequently, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "houses = np.array(houses)\n",
    "print(houses.shape)\n",
    "\n",
    "weights = np.array(weights)\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that our dummy data set is now represented by a two-dimensional matrix which has 5 rows (corresponding to the number of houses) and 5 columns (corresponding to the number of features). Our `weights` variable, on the other hand is a single-dimensional vector, instead of a matrix, of consisting of five numbers (scalars). We are now ready to redefine our Perceptron blueprint to be able to deal with such arrays more efficiently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def v_sigmoid(x):\n",
    "    # element-wise; eats any tensor\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class VectorizedPerceptron:\n",
    "    def __init__(self, nb_features=None):\n",
    "        self.weights = None\n",
    "        self.nb_features = nb_features\n",
    "    \n",
    "    def set_weights(self, weights=None):\n",
    "        if isinstance(weights, np.ndarray):\n",
    "            self.weights = weights\n",
    "        else:\n",
    "            self.weights = np.ones(self.nb_features)\n",
    "    \n",
    "    def predict(self, feature_vectors, squash=False):\n",
    "        scores = np.multiply(feature_vectors, self.weights).sum(axis=-1)\n",
    "        \n",
    "        if squash:\n",
    "            scores = v_sigmoid(scores)\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, once you get used to such vectorized notation, vectorized code becomes much easier to read than stacks of for-loops. The result is the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v_perceptron = VectorizedPerceptron()\n",
    "v_perceptron.set_weights(weights)\n",
    "\n",
    "print(perceptron.predict(houses))\n",
    "print(perceptron.predict(houses, squash=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But is this faster? Let us create an artificial, large house data set to check this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "houses = np.random.uniform(low=-0.05, high=0.05, size=(1000, 500))\n",
    "print(houses.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# simple:\n",
    "weights = list(np.random.uniform(low=-0.05, high=0.05, size=(500)))\n",
    "perceptron = Perceptron(nb_features=500)\n",
    "perceptron.set_weights(weights)\n",
    "\n",
    "# vectorized:\n",
    "v_perceptron = VectorizedPerceptron(nb_features=500)\n",
    "v_perceptron.set_weights(np.array(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can time the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit perceptron.predict(houses, squash=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit v_perceptron.predict(houses, squash=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jip, it seems to be quite a bit faster -- and believe me, the difference grows even larger for larger data sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first implement a pre-historic approach to optimizing our perceptron. We first load the Boston Housing Prices Dataset that ships with `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this data set has information for 506 houses across 13 numerical features. The target vector which we like to fit is a list of 506 prices associated with each house:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start again with a stripped down version of our `Perceptron` class: upon instantiation, objects from this class will create a list of weight parameters, or really small values randomly sampled from a uniform distribution betwee: -0.05 and +0.05:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, nb_features=None):\n",
    "        np.random.seed(156651)\n",
    "        self.weights = np.random.uniform(low=-.05, high=0.05,\n",
    "                                         size=nb_features)\n",
    "    \n",
    "    def predict(self, feature_vectors):\n",
    "        return np.multiply(feature_vectors, self.weights).sum(axis=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, our `Perceptron` has a `predict` method, which we can use to obtain house prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "perceptron = Perceptron()\n",
    "prices = perceptron.predict(X)\n",
    "print(prices.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to measure the current quality of our system, we need a **loss function** or **objective function**: later, we will try to get down the loss returned by the function. As a loss function, we start off with the relatively **mean squared error**: it compares the proces outputted by the system and the **gold standard** (in the `y` vector) and returns the mean of squares with respect to the difference between each predicted price and its gold standard equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(y_gold, y_pred):\n",
    "    return np.mean((y_gold - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us find out how large the loss is for our current model, which has randomly initialized parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = perceptron.predict(X)\n",
    "print(mean_squared_error(preds, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that's huge! We are now ready to add a naive `fit` method to optimize the parameters of our perceptron. In each **epoch** (i.e. one pass over the entire data set), we loop over each parameter individually and test two different situations: *hypothesis a*, in which we slightly increase it (with a small rate `learning_rate`) and *hypothesis b*, in which we slightly decrease the weight under scrutiny (again with a small `learning` rate). Then, we adjust the parameters in the light of whichever hypothesis gave the best result (i.e. maximally minimized the loss). That goes like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, nb_features=None):\n",
    "        self.nb_features = nb_features\n",
    "        np.random.seed(156651)\n",
    "        self.weights = np.array(np.random.uniform(low=-.05, high=0.5,\n",
    "                                         size=self.nb_features))\n",
    "    \n",
    "    def predict(self, feature_vectors, weights=None):\n",
    "        if not isinstance(weights, np.ndarray):\n",
    "            return np.multiply(feature_vectors, self.weights).sum(axis=-1)\n",
    "        else:\n",
    "            return np.multiply(feature_vectors, weights).sum(axis=-1)\n",
    "    \n",
    "    def fit(self, X, y, learning_rate=0.1, nb_epochs=10):\n",
    "        losses = []\n",
    "        \n",
    "        for e in range(nb_epochs):\n",
    "            if e % 100 == 0 and e:\n",
    "                learning_rate *= 0.9\n",
    "            \n",
    "            for idx in range(self.nb_features):\n",
    "                # hypothesis a: we increase the parameter:\n",
    "                weights_plus = self.weights.copy()\n",
    "                weights_plus[idx] += learning_rate\n",
    "\n",
    "                # hypothesis b: we decrease the parameter:\n",
    "                weights_minus = self.weights.copy()\n",
    "                weights_minus[idx] -= learning_rate\n",
    "                \n",
    "                # we obtain the predictions for both hypotheses a and b:\n",
    "                plus_preds = self.predict(X, weights = weights_plus)\n",
    "                minus_preds = self.predict(X, weights = weights_minus)\n",
    "                \n",
    "                # we obtain the predictions for both hypotheses a and b:\n",
    "                plus_loss = mean_squared_error(plus_preds, y)\n",
    "                minus_loss = mean_squared_error(minus_preds, y)\n",
    "                \n",
    "                # we adjust \n",
    "                if plus_loss < minus_loss:\n",
    "                    self.weights = weights_plus.copy()\n",
    "                elif minus_loss < plus_loss:\n",
    "                    self.weights = weights_minus.copy()\n",
    "            \n",
    "            # we calculate the loss\n",
    "            loss = mean_squared_error(self.predict(X), y)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            if e and e % 20 == 0:\n",
    "                print('Loss after epoch #', e + 1, '->', loss)\n",
    "        \n",
    "        return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now apply this fit method with a small enough learning rate, you see that we are able to gradually push down the loss in the subsequent epochs, until the loss curve staurates and stabilizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = Perceptron(nb_features=X.shape[1])\n",
    "p.predict(X)\n",
    "losses = p.fit(X, y, nb_epochs=500, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A graphical representation of the loss curve is easy enough to obtain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do It Yourself:\n",
    "\n",
    "Add some simple modifications to the code above. Note that these exercises in fact introduce some key concepts that are very central in modern deep learning research.\n",
    "\n",
    "* Adapt our Perceptron to show an adaptive, **decreasing learning rate**: after a number of epochs (e.g. 100), it would make sense to decrease the learning steps taken, for instance by a factor of three.\n",
    "* Implement a form of **early stopping**: stop the training procedure, if the loss doesn't significantly go down anymore for a fixed number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Perceptron in Theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Our previous optimization method worked, but was ugly and slow. In this part of the tutorial, we turn to antoher optimization technique that is the main workhorse behind modern representation learning (i.e. gradient descent). We implement the same perceptron as above in **theano**, which is one of the most popular Deep Learning toolkits currently available. We import theano and initialize our random seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T # do you know what a tensor is?\n",
    "rng = np.random\n",
    "rng.seed(156651)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by instantiating the input matrix which we will use (our `X` matrix of house features) and the output vector which we would like our model to return, i.e. the list of prices for each house. Both consist of floats, hence the `f` at the beginning of `fmatrix` and `fvector`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_input = T.fmatrix('train_input')\n",
    "train_target = T.fvector('train_output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is important is that at this point, both `train_input` and `train_target` are purely **symbolic variables** that don't have an actual value yet. These variables are mere blueprints or abstract placeholders for when we will actually start using the model. Below, we instantiate our weight matrix containing our parameters. This is not a symbolic variable of course; it will already have actual values in it, because we initialize it randomly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = np.asarray(rng.uniform(low=-0.05, high=0.05,\n",
    "                                 size=(X.shape[1])),\n",
    "                     dtype='float32')\n",
    "print(weights)\n",
    "print(weights.shape)\n",
    "\n",
    "W = theano.shared(value=weights, name='W', borrow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We make use of a theano variable which can 'shared' and 'borrowed', because every component of our code and system will need access to these numbers during the optimization. We make use of 'float32' floating point numbers, because currently those are they only ones -- apart from integers, of course -- which are fully supported on the GPU architectures on which neural networks are typically trained. We are now ready to define the graph which will represent our model. In our case of a plain regression model, the prediction is simple enough:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = (train_input * W).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, `prediction` is a mere symbolic variable or a node in the model **graph** we are constructing: to turn into an actual function which we use, we need to **compile** it. In `theano`, we can do that as follows, by specifying both the input and output that this function will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict = theano.function([train_input], prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice that compiling the graph takes a short while? That is because, under the hood, `theano` will try to optimize a function quite heavily, so that it will be fast to use afterwards. We can now already apply this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.array(X, dtype='float32')\n",
    "predictions = predict(X)\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we obtain the output vectors we want: their shape already fits our expectations (i.e. a price for each house), but their values will still be worhtless, because our weights are randomly initialized. To find out how well we are currently doing, we need define a function which we can use to measure the cost for a certain state of the model (as we did above), by comparing the current output of the model to the ideal target values which we would have liked to obtain. The cost or objective which we use is, again, the mean squared error. In `theano`, calculating this cost runs largely parallel to the code for `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "silver_predictions = T.fvector('silver')\n",
    "gold_predictions = T.fvector('gold')\n",
    "y = np.array(y, dtype='float32')\n",
    "\n",
    "cost = T.mean((silver_predictions - gold_predictions) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cost` variable too can be compiled into an actual function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mse = theano.function([silver_predictions, gold_predictions], cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we apply the `mse` function, we see that our cost is huge at this point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(mse(predictions, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check whether the output of our theano function is the same as our numpy implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(y_gold, y_pred):\n",
    "    return ((y_gold - y_pred) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_squared_error(predictions, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to let the magic happen: in the following block we define a variable that will contain the gradients of our parameters, with respect to the cost function which we defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "symbolic_cost = T.mean((prediction - train_target) ** 2)\n",
    "gradients = T.grad(symbolic_cost, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazingly, we can now check out the current gradients of each of the 13 parameters in our system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_grads = theano.function([train_input, train_target], gradients)\n",
    "print(get_grads(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these gradients, we don't need to check both hypothesis a and b anymore for the parameters. Moreover, we can make use of the relative strength of the gradients, allowing for a more flexible update rule. We are now ready to start the optimization process. First, we need to specify a `learning_rate` factor and the number of **epochs** we would to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = np.float32(0.0000001)\n",
    "nb_epoch = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 'official' theano, the training function would be something like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "updates = [(W, W - np.float32(learning_rate) * gradients)]\n",
    "\n",
    "train_model = theano.function(\n",
    "        inputs=[train_input, train_target],\n",
    "        outputs=symbolic_cost,\n",
    "        updates=updates,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we first need to specify an update rule, which will reflect our training mechanism. Then, we compile a function, that takes our `X` and `y` as inputs (`[train_input, train_target]`) and returns the cost (the result of `symbolic`) as a result. Additionally, each time we call this function, our `updates` will be carried out, i.e. the parameters will be adjusted in the light of the respective gradients. We are now ready to launch the model for a number of epochs, and keep track of the -- hopefully decreasing! -- loss in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "for e in range(nb_epoch):\n",
    "    c = train_model(X[start : end], y[start : end])\n",
    "    print('-> current loss:', c)\n",
    "    losses.append(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! The loss is clearly decreasing and saturates rather quickly. We can graphically inspect the **loss curve** as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To make this even more intuitive, we will also implement the update rule ourselves. First, we need to re-initialize our shared variable (to make sure that we start from the same baseline):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rng.seed(156651)\n",
    "weights = np.asarray(rng.uniform(low=-0.05, high=0.05,\n",
    "                                 size=(X.shape[1])),\n",
    "                     dtype='float32')\n",
    "W.set_value(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start looping and each time we update our parameters in the light of their respective gradients using a simple **update rule**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "for e in range(nb_epoch):\n",
    "    predictions = predict(X)\n",
    "    curr_loss = mse(predictions, y)\n",
    "    if e % 50 == 0:\n",
    "        print('current loss ->', curr_loss)\n",
    "    losses.append(curr_loss)\n",
    "    \n",
    "    # the update mechanism\n",
    "    curr_grads = get_grads(X, y)\n",
    "    curr_weights = W.get_value(borrow=True)\n",
    "    curr_weights = curr_weights - learning_rate * curr_grads # the update rule\n",
    "    W.set_value(curr_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we obtain a loss curve that looks very similar to our previous one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should give you an intuition of what it means to do gradient descent and this can be efficiently implemented using a library like Theano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIY\n",
    "\n",
    "What we implemented above is 'traditional' gradient descent, where we base our gradient updates on the current loss for the entire dataset at once. In present-day research in neural networks, most data sets are too large too fit in memory, especially if you train networks on GPU which have much more restricted memory capabilities (which sadly compensates their incredible speed). Therefore, scientist nowadays turn to either **stochastic gradient descent** (SGD), where the updates are based on the loss of a smaller sample of the data. Most commonly, people use **minibatch gradient descent**, where each time the loss is calculated for a small random batch of training instances. Common batch sizes range from 30 to 150 (depending on how much data you can fit into memory at once). **Exercise**: implement the SGD training approach with a minibatch size of 25. In each epoch, you should loop over the entire data set in minibatches of 25 training instances from `X` and `y`. Calculate the loss for each batch and update the gradients in the light of the gradients you obtain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Perceptron in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(For the sake of reference, I include a similar implementation of our perceptron in `Tensorflow`, but we probably won't have tim eto work through block by block...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "X = np.array(X, dtype='float32')\n",
    "y = np.array(y, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# purely \"symbolic variable\"\n",
    "train_input = tf.placeholder(tf.float32, [None, X.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_output = tf.placeholder(tf.float32, [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rng.seed(156651)\n",
    "weights = tf.Variable(np.asarray(rng.uniform(low=-0.05, high=0.05,\n",
    "                                 size=(X.shape[1])),\n",
    "                     dtype='float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_output = tf.reduce_sum(tf.mul(train_input, weights), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mse_cost = tf.reduce_mean(tf.square(model_output - train_output),\n",
    "                          reduction_indices=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.0000001).minimize(mse_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = sess.run(model_output, feed_dict={train_input: X})\n",
    "print(mean_squared_error(f, y))\n",
    "g = sess.run(mse_cost, feed_dict={train_input: X, train_output: y})\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "for i in range(500):\n",
    "    c = sess.run(mse_cost, feed_dict={train_input: X, train_output: y})\n",
    "    losses.append(c)\n",
    "    if i % 50 == 0:\n",
    "        print(c)\n",
    "    sess.run(train_step, feed_dict={train_input: X, train_output: y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
