{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by importing some of the libraries we will need and set up our notebook session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Basic Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here goes the blueprint of our perceptron, encoded as a plain Python object, as well as a single auxiliary function. Below we will walk through the details of each code components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, nb_features=None):\n",
    "        self.weights = None\n",
    "        self.nb_features = nb_features # i.e. the nb of incoming connections from other neurons\n",
    "    \n",
    "    def set_weights(self, weights=None):\n",
    "        if weights:\n",
    "            self.weights = weights\n",
    "        else:\n",
    "            self.weights = np.ones(self.nb_features)\n",
    "    \n",
    "    def predict(self, feature_vectors, squash=False):\n",
    "        scores = []\n",
    "        for fv in feature_vectors:\n",
    "            s = 0.0\n",
    "            for i in range(self.nb_features):\n",
    "                s += self.weights[i] * fv[i]\n",
    "            scores.append(s)\n",
    "        \n",
    "        if squash:\n",
    "            scores = [sigmoid(v) for v in scores]\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by predicting house prices in a really naive way, i.e. we assume that all characteristics are equally important, each having a (positive) weight of 1. We characterize each house along 5 integer variables, namely:\n",
    "* the number of doors\n",
    "* surface (in square meters)\n",
    "* the number of bedrooms\n",
    "* the number of bathrooms\n",
    "* its age (in years)\n",
    "\n",
    "First, we initialize our perceptron, and we specify that we will use 5 input features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perceptron = Perceptron(nb_features=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set the weights of our perceptron; by default, our perceptron will assign an equal, positive weight of 1.0 to each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "perceptron.set_weights()\n",
    "print(perceptron.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a dummy data set of 5 houses, which are represented as a fixed length vector of five integers (the index of which corresponds to the bullet list above): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "houses = [[2, 3, 5, 1, 8],\n",
    "          [1, 2, 1, 3, 5],\n",
    "          [4, 4, 2, 1, 3],\n",
    "          [11, 6, 8, 9, 8],\n",
    "          [9, 10, 8, 1, 30]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this data set, the last houses have higher a number of doors, bathrooms etc., so that they would have to predict higher prices. We now feed this data set to the `predict()` function of our perceptron (cf. standard `sklearn` naming conventions), and we have it predict a the price for each house:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19.0, 12.0, 14.0, 42.0, 58.0]\n"
     ]
    }
   ],
   "source": [
    "prices = perceptron.predict(houses)\n",
    "print(prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the see that for instance the pentultimate house has a relatively higher price. Of course, our feature weighting is now ridiculously naive. Clearly,\n",
    "* the number of bathrooms should matter relative more than the number of doors\n",
    "* the age of a house should negatively affect the total price\n",
    "\n",
    "We can now fix this by setting a more sensible weight vector, which should have five entries (one for each feature). Using these new weights, we should now obtain a more accurate estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.6, 3.3, 6.6, 21.4, 5.799999999999997]\n"
     ]
    }
   ],
   "source": [
    "weights = [0.5, # doors\n",
    "           0.9, # surface\n",
    "           0.8, # bedrooms\n",
    "           0.9, # bathrooms\n",
    "           -0.5] # age\n",
    "perceptron.set_weights(weights)\n",
    "print(perceptron.predict(houses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the price of a house is simple enough and comes down to calculating the **weighted sum** of the feature values. We now see, for instance, that the final house will be valued much less than the pentultimate house in the list, based on the assumption that, while is it much larger, it is also is much older and will therefore require much more investments. Note that the prices predicted are not in an actual currency and vary wildly. In many applications, it is very common to **normalize** the predictions of a perceptron and 'squash' them to a range between 0 and 1. Historically, the **sigmoid** function has often been for this. By setting `squash = True`, we can implement this behaviour in our perceptron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9900481981330957, 0.9644288107273639, 0.9986414800495711, 0.9999999994917257, 0.9969815836752917]\n"
     ]
    }
   ],
   "source": [
    "print(perceptron.predict(houses, squash=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see that we obtain predictions which are neatly between 0 and 1. In the context of deep learning, functions such as the sigmoid function are often called **activation** functions, because they control how strongly a particular neuron will be activated. In our perceptron, we have a single output neuron, the activation of which is controlled via a sigmoid activation. We call such an activation **element-wise**, is they are applied to each element in a list individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speeding up our perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron which we created will be slow, because, right now, it only relies on traditional, unoptimized Python code. Inspect for instance the code block in the `predict()` function above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict():\n",
    "    for fv in feature_vectors:\n",
    "        s = 0.0\n",
    "        for i in range(self.nb_features):\n",
    "            s += self.weights[i] * fv[i]\n",
    "        scores.append(s)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The many `for`-loops in this code will make it extremely slow to run. In Deep Learning (and in so many other fields), such code will be too slow to be usable in practice: it is much more common to used vectorized routines from specialized libraries, such as `numpy`, which can easily replace our naive `for`-loops. To be able to vectorize our code more efficiently, we will have to convert our Python lists to `numpy` arrays, which essentially are matrix objects, that have a **`shape`**. As will become clear below, the `shape` is an extremely important property of `numpy` arrays, and you will want to keep track of it frequently, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5)\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "houses = np.array(houses)\n",
    "print(houses.shape)\n",
    "\n",
    "weights = np.array(weights)\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that our dummy data set is now represented by a two-dimensional matrix which has 5 rows (corresponding to the number of houses) and 5 columns (corresponding to the number of features). Our `weights` variable, on the other hand is a single-dimensional vector, instead of a matrix, of consisting of five numbers (scalars). We are now ready to redefine our Perceptron blueprint to be able to deal with such arrays more efficiently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def v_sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class VectorizedPerceptron:\n",
    "    def __init__(self, nb_features=None):\n",
    "        self.weights = None\n",
    "        self.nb_features = nb_features\n",
    "    \n",
    "    def set_weights(self, weights=None):\n",
    "        if isinstance(weights, np.ndarray):\n",
    "            self.weights = weights\n",
    "        else:\n",
    "            self.weights = np.ones(self.nb_features)\n",
    "    \n",
    "    def predict(self, feature_vectors, squash=False):\n",
    "        scores = np.dot(feature_vectors, self.weights)\n",
    "        \n",
    "        if squash:\n",
    "            scores = v_sigmoid(scores)\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, once you get used to such vectorized notation, vectorized code becomes much easier to read than stacks of for-loops. The result is the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.6, 3.3, 6.6, 21.4, 5.799999999999997]\n",
      "[0.9900481981330957, 0.9644288107273639, 0.9986414800495711, 0.9999999994917257, 0.9969815836752917]\n"
     ]
    }
   ],
   "source": [
    "v_perceptron = VectorizedPerceptron()\n",
    "v_perceptron.set_weights(weights)\n",
    "print(perceptron.predict(houses))\n",
    "print(perceptron.predict(houses, squash=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But is this faster? Let us create an artificial, large house data set to check this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 500)\n"
     ]
    }
   ],
   "source": [
    "houses = np.random.uniform(low=-0.05, high=0.05, size=(1000, 500))\n",
    "print(houses.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1306,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# simple:\n",
    "weights = list(np.random.randint(5, size=(500)))\n",
    "perceptron = Perceptron(nb_features=500)\n",
    "perceptron.set_weights(weights)\n",
    "\n",
    "# vectorized:\n",
    "v_perceptron = VectorizedPerceptron(nb_features=500)\n",
    "v_perceptron.set_weights(np.array(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can time the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit perceptron.predict(houses, squash=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1292-b782d1c53ecf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'timeit v_perceptron.predict(houses, squash=True)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/mike/miniconda2/envs/py27/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2161\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2162\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2163\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2165\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mike/miniconda2/envs/py27/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2082\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2083\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2084\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2085\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m/Users/mike/miniconda2/envs/py27/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mike/miniconda2/envs/py27/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[0mnumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1041\u001b[0;31m                 \u001b[0mtime_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1042\u001b[0m                 \u001b[0mworst_tuning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworst_tuning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mike/miniconda2/envs/py27/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1288-7a8eed62bc32>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, feature_vectors, squash)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msquash\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mike/miniconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/ops/variables.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RunOp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RunOp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mike/miniconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/ops/variables.pyc\u001b[0m in \u001b[0;36m_RunOp\u001b[0;34m(operator, a, b)\u001b[0m\n\u001b[1;32m    624\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_AsTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_AsTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mike/miniconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.pyc\u001b[0m in \u001b[0;36mr_binary_op_wrapper\u001b[0;34m(y, x)\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mclazz_object\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mike/miniconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.pyc\u001b[0m in \u001b[0;36m_mul_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    771\u001b[0m   \u001b[0mis_tensor_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mis_tensor_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Case: Dense * Sparse.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mike/miniconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.pyc\u001b[0m in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1332\u001b[0m     \u001b[0mA\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m   \"\"\"\n\u001b[0;32m-> 1334\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op_def_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mike/miniconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.pyc\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    702\u001b[0m           op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    703\u001b[0m                            \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                            op_def=op_def)\n\u001b[0m\u001b[1;32m    705\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m           return _Restructure(ops.convert_n_to_tensor(outputs),\n",
      "\u001b[0;32m/Users/mike/miniconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2236\u001b[0m       \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2238\u001b[0;31m     \u001b[0mnode_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_NodeDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2240\u001b[0m     \u001b[0;31m# Apply a kernel label if one has been specified for this op_type.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mike/miniconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_NodeDef\u001b[0;34m(op_type, name, device, attrs)\u001b[0m\n\u001b[1;32m   1085\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mgraph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNodeDef\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m   \"\"\"\n\u001b[0;32m-> 1087\u001b[0;31m   \u001b[0mnode_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNodeDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1088\u001b[0m   \u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m   \u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mike/miniconda2/envs/py27/lib/python2.7/site-packages/google/protobuf/internal/python_message.pyc\u001b[0m in \u001b[0;36minit\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_byte_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_byte_size_dirty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%timeit v_perceptron.predict(houses, squash=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jip, it seems to be quite a bit faster -- and believe me, the difference grows even larger for larger data sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-historic optimization. We load the Boston Housing Prices Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "(506,)\n",
      "[ 24.   21.6  34.7  33.4  36.2  28.7  22.9  27.1  16.5  18.9  15.   18.9\n",
      "  21.7  20.4  18.2  19.9  23.1  17.5  20.2  18.2  13.6  19.6  15.2  14.5\n",
      "  15.6  13.9  16.6  14.8  18.4  21.   12.7  14.5  13.2  13.1  13.5  18.9\n",
      "  20.   21.   24.7  30.8  34.9  26.6  25.3  24.7  21.2  19.3  20.   16.6\n",
      "  14.4  19.4  19.7  20.5  25.   23.4  18.9  35.4  24.7  31.6  23.3  19.6\n",
      "  18.7  16.   22.2  25.   33.   23.5  19.4  22.   17.4  20.9  24.2  21.7\n",
      "  22.8  23.4  24.1  21.4  20.   20.8  21.2  20.3  28.   23.9  24.8  22.9\n",
      "  23.9  26.6  22.5  22.2  23.6  28.7  22.6  22.   22.9  25.   20.6  28.4\n",
      "  21.4  38.7  43.8  33.2  27.5  26.5  18.6  19.3  20.1  19.5  19.5  20.4\n",
      "  19.8  19.4  21.7  22.8  18.8  18.7  18.5  18.3  21.2  19.2  20.4  19.3\n",
      "  22.   20.3  20.5  17.3  18.8  21.4  15.7  16.2  18.   14.3  19.2  19.6\n",
      "  23.   18.4  15.6  18.1  17.4  17.1  13.3  17.8  14.   14.4  13.4  15.6\n",
      "  11.8  13.8  15.6  14.6  17.8  15.4  21.5  19.6  15.3  19.4  17.   15.6\n",
      "  13.1  41.3  24.3  23.3  27.   50.   50.   50.   22.7  25.   50.   23.8\n",
      "  23.8  22.3  17.4  19.1  23.1  23.6  22.6  29.4  23.2  24.6  29.9  37.2\n",
      "  39.8  36.2  37.9  32.5  26.4  29.6  50.   32.   29.8  34.9  37.   30.5\n",
      "  36.4  31.1  29.1  50.   33.3  30.3  34.6  34.9  32.9  24.1  42.3  48.5\n",
      "  50.   22.6  24.4  22.5  24.4  20.   21.7  19.3  22.4  28.1  23.7  25.\n",
      "  23.3  28.7  21.5  23.   26.7  21.7  27.5  30.1  44.8  50.   37.6  31.6\n",
      "  46.7  31.5  24.3  31.7  41.7  48.3  29.   24.   25.1  31.5  23.7  23.3\n",
      "  22.   20.1  22.2  23.7  17.6  18.5  24.3  20.5  24.5  26.2  24.4  24.8\n",
      "  29.6  42.8  21.9  20.9  44.   50.   36.   30.1  33.8  43.1  48.8  31.\n",
      "  36.5  22.8  30.7  50.   43.5  20.7  21.1  25.2  24.4  35.2  32.4  32.\n",
      "  33.2  33.1  29.1  35.1  45.4  35.4  46.   50.   32.2  22.   20.1  23.2\n",
      "  22.3  24.8  28.5  37.3  27.9  23.9  21.7  28.6  27.1  20.3  22.5  29.\n",
      "  24.8  22.   26.4  33.1  36.1  28.4  33.4  28.2  22.8  20.3  16.1  22.1\n",
      "  19.4  21.6  23.8  16.2  17.8  19.8  23.1  21.   23.8  23.1  20.4  18.5\n",
      "  25.   24.6  23.   22.2  19.3  22.6  19.8  17.1  19.4  22.2  20.7  21.1\n",
      "  19.5  18.5  20.6  19.   18.7  32.7  16.5  23.9  31.2  17.5  17.2  23.1\n",
      "  24.5  26.6  22.9  24.1  18.6  30.1  18.2  20.6  17.8  21.7  22.7  22.6\n",
      "  25.   19.9  20.8  16.8  21.9  27.5  21.9  23.1  50.   50.   50.   50.\n",
      "  50.   13.8  13.8  15.   13.9  13.3  13.1  10.2  10.4  10.9  11.3  12.3\n",
      "   8.8   7.2  10.5   7.4  10.2  11.5  15.1  23.2   9.7  13.8  12.7  13.1\n",
      "  12.5   8.5   5.    6.3   5.6   7.2  12.1   8.3   8.5   5.   11.9  27.9\n",
      "  17.2  27.5  15.   17.2  17.9  16.3   7.    7.2   7.5  10.4   8.8   8.4\n",
      "  16.7  14.2  20.8  13.4  11.7   8.3  10.2  10.9  11.    9.5  14.5  14.1\n",
      "  16.1  14.3  11.7  13.4   9.6   8.7   8.4  12.8  10.5  17.1  18.4  15.4\n",
      "  10.8  11.8  14.9  12.6  14.1  13.   13.4  15.2  16.1  17.8  14.9  14.1\n",
      "  12.7  13.5  14.9  20.   16.4  17.7  19.5  20.2  21.4  19.9  19.   19.1\n",
      "  19.1  20.1  19.9  19.6  23.2  29.8  13.8  13.3  16.7  12.   14.6  21.4\n",
      "  23.   23.7  25.   21.8  20.6  21.2  19.1  20.6  15.2   7.    8.1  13.6\n",
      "  20.1  21.8  24.5  23.1  19.7  18.3  21.2  17.5  16.8  22.4  20.6  23.9\n",
      "  22.   11.9]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1308,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, nb_features=None):\n",
    "        self.weights = np.random.uniform(low=-.05, high=0.05,\n",
    "                                         size=nb_features)\n",
    "    \n",
    "    def predict(self, feature_vectors):\n",
    "        return np.dot(feature_vectors, self.weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.01576087e-04  -8.58919236e-01  -1.10227969e-01 ...,  -7.30081351e-01\n",
      "   -1.89391692e+01  -2.37634322e-01]\n",
      " [ -1.30317135e-03   0.00000000e+00  -3.37364389e-01 ...,  -8.49375689e-01\n",
      "   -1.89391692e+01  -4.36140101e-01]\n",
      " [ -1.30221700e-03   0.00000000e+00  -3.37364389e-01 ...,  -8.49375689e-01\n",
      "   -1.87449580e+01  -1.92302473e-01]\n",
      " ..., \n",
      " [ -2.89932960e-03   0.00000000e+00  -5.69272583e-01 ...,  -1.00207244e+00\n",
      "   -1.89391692e+01  -2.69128027e-01]\n",
      " [ -5.22938662e-03   0.00000000e+00  -5.69272583e-01 ...,  -1.00207244e+00\n",
      "   -1.87745430e+01  -3.09210925e-01]\n",
      " [ -2.26229783e-03   0.00000000e+00  -5.69272583e-01 ...,  -1.00207244e+00\n",
      "   -1.89391692e+01  -3.76015755e-01]]\n"
     ]
    }
   ],
   "source": [
    "p = Perceptron()\n",
    "prices = p.predict(X)\n",
    "print(prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1310,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(y_gold, y_pred):\n",
    "    return ((y_gold - y_pred) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1311,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, nb_features=None):\n",
    "        self.nb_features = nb_features\n",
    "        self.weights = np.random.uniform(low=-.05, high=0.5,\n",
    "                                         size=self.nb_features)\n",
    "    \n",
    "    def predict(self, feature_vectors, weights=None):\n",
    "        if not isinstance(weights, np.ndarray):\n",
    "            return np.dot(feature_vectors, self.weights)\n",
    "        else:\n",
    "            return np.dot(feature_vectors, weights)\n",
    "    \n",
    "    def fit(self, X, y, learning_rate=0.1, nb_epochs=10):\n",
    "        losses = []\n",
    "        \n",
    "        for e in range(nb_epochs):\n",
    "            for idx in range(self.nb_features):\n",
    "                weights_plus = self.weights.copy()\n",
    "                weights_plus[idx] += learning_rate\n",
    "\n",
    "                weights_minus = self.weights.copy()\n",
    "                weights_minus[idx] -= learning_rate\n",
    "\n",
    "                plus_preds = self.predict(X, weights = weights_plus)\n",
    "                minus_preds = self.predict(X, weights = weights_minus)\n",
    "\n",
    "                plus_loss = mean_squared_error(plus_preds, y)\n",
    "                minus_loss = mean_squared_error(minus_preds, y)\n",
    "                \n",
    "                if plus_loss < minus_loss:\n",
    "                    self.weights = weights_plus.copy()\n",
    "                else:\n",
    "                    self.weights = weights_minus.copy()\n",
    "            \n",
    "            loss = mean_squared_error(self.predict(X), y)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            if e and e % 20 == 0:\n",
    "                print('Loss after epoch #', e + 1, '->', loss)\n",
    "        \n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch # 21 -> 91.3041567432\n",
      "Loss after epoch # 41 -> 56.0250231736\n",
      "Loss after epoch # 61 -> 51.3953119085\n",
      "Loss after epoch # 81 -> 45.8737799837\n"
     ]
    }
   ],
   "source": [
    "p = Perceptron(nb_features=X.shape[1])\n",
    "losses = p.fit(X, y, nb_epochs=100, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14b99fa90>]"
      ]
     },
     "execution_count": 1313,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4eRgZmZtODmYmVkb/x+hGYxnPxbmhAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14b8ffed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do It Yourself:\n",
    "\n",
    "* Adapt our Perceptron to show an adaptive, **decreasing learning rate**: after a number of epochs (e.g. 100), it would make sense to decrease the learning, for instance by a factor of three.\n",
    "* Implement a form of **early stopping**: stop the training procedure, if the loss doesn't significantly go down anymore for a number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Perceptron in Theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We import theano and initialize our random seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "rng = np.random\n",
    "rng.seed(156651)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by instantiating the input matrix which we will use (our X matrix of house features) and the output vector which we could like our model to return, i.e. the list of prices for each house. Both consist of floats, hence the `f` at the beginning of `fmatrix` and `fvector`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1315,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_input = T.fmatrix('train_input')\n",
    "train_target = T.fvector('train_output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is important is that at this point, both `train_input` and `train_target` are purely symbolic variables that don't have an actual value yet. These variables are mere placeholders for when we will actually start using the model. Below, we instantiate our weight metrix containing our parameters. This is not a symbolic variable: it will alreayd have actual values in it, because we initialize it randomly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04963301  0.04475696 -0.02963052 -0.01821073 -0.04093222  0.0021752\n",
      "  0.03168588 -0.0228706   0.04216252 -0.00994067 -0.01141965 -0.02260192\n",
      " -0.01913692]\n",
      "(13,)\n"
     ]
    }
   ],
   "source": [
    "weights = np.asarray(rng.uniform(low=-0.05, high=0.05,\n",
    "                                 size=(X.shape[1])),\n",
    "                     dtype='float32')\n",
    "print(weights)\n",
    "print(weights.shape)\n",
    "\n",
    "W = theano.shared(value=weights, name='W', borrow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We make use of a theano variable which can 'shared' and 'borrowed', because every component of our code and system will need access to these numbers during the optimization. We make use of 'float32' floating point numbers, because currently those are they only ones -- apart from integers -- which are fully supported on the GPU architectures on which neural networks are typically trained. We are now ready to define the graph which will represent our model. In our case of a plain regression model, the prediction is simple enough:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1317,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = (train_input * W).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, `prediction` is a mere symbolic variable or a node in the modle graph we are constructing: to turn into an actual function which we use, we need to **compile** it. In `theano`, we can do that as follows, by specifying both the input and output that this function will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict = theano.function([train_input], prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice that compiling the graph takes a short while? That is because `theano` will try to optimize a function quite heavily, so that it will be fast to use afterwards. We can now already apply this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506,)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(X, dtype='float32')\n",
    "predictions = predict(X)\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we obtain the output vector: its shape already fits our expectations (i.e. a price for each house), but its values will still be worhtless, because our weights are randomly initialized. To find out how well we are currently doing, we need define a function which we can use to measure the cost for a certain state of the model (as we did above), by comparing the current output of the model to the ideal target values which we would have liked to obtain. The cost or objective which we use is the mean squared error. In `theano`, calculating this cost runs largely parallel to the code for `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "silver_predictions = T.fvector('silver')\n",
    "gold_predictions = T.fvector('gold')\n",
    "y = np.array(y, dtype='float32')\n",
    "\n",
    "cost = T.mean((silver_predictions - gold_predictions) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The symbolic `cost` variable too can be compiled into an actual function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mse = theano.function([silver_predictions, gold_predictions], cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we apply the `mse` function, we see that our cost is huge at this point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1146.99194336\n"
     ]
    }
   ],
   "source": [
    "print(mse(predictions, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1323,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(y_gold, y_pred):\n",
    "    return ((y_gold - y_pred) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1146.9921"
      ]
     },
     "execution_count": 1324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(predictions, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to let the magic happen: in the following block we define a variable that will contain the gradients of our parameters, with respect to the cost function which we defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "symbolic_cost = T.mean((prediction - train_target) ** 2)\n",
    "gradients = T.grad(symbolic_cost, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazingly, we can now check out the current gradients of each of the 13 parameters in our system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -1.80837784e+02  -8.57805847e+02  -6.75006226e+02  -5.32577467e+00\n",
      "  -3.53757362e+01  -4.18336609e+02  -4.28877832e+03  -2.55309296e+02\n",
      "  -5.72623352e+02  -2.54460781e+04  -1.18703381e+03  -2.40647793e+04\n",
      "  -7.33394226e+02]\n"
     ]
    }
   ],
   "source": [
    "get_grads = theano.function([train_input, train_target], gradients)\n",
    "print(get_grads(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to start the optimization process. First, we need to specify a `learning_rate` factor and the number of **epochs** we would to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1327,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = np.float32(0.0000001)\n",
    "nb_epoch = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13,)\n"
     ]
    }
   ],
   "source": [
    "print(W.get_value().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "updates = [(W, W - np.float32(learning_rate) * gradients)]\n",
    "\n",
    "train_model = theano.function(\n",
    "        inputs=[train_input, train_target],\n",
    "        outputs=symbolic_cost,\n",
    "        updates=updates,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> current loss: 1146.99194336\n",
      "-> current loss: 1026.00390625\n",
      "-> current loss: 919.606018066\n",
      "-> current loss: 826.029541016\n",
      "-> current loss: 743.720031738\n",
      "-> current loss: 671.311889648\n",
      "-> current loss: 607.604858398\n",
      "-> current loss: 551.544433594\n",
      "-> current loss: 502.203826904\n",
      "-> current loss: 458.768859863\n",
      "-> current loss: 420.523895264\n",
      "-> current loss: 386.840270996\n",
      "-> current loss: 357.165405273\n",
      "-> current loss: 331.013793945\n",
      "-> current loss: 307.958892822\n",
      "-> current loss: 287.625732422\n",
      "-> current loss: 269.684967041\n",
      "-> current loss: 253.847076416\n",
      "-> current loss: 239.857681274\n",
      "-> current loss: 227.493286133\n",
      "-> current loss: 216.557449341\n",
      "-> current loss: 206.877593994\n",
      "-> current loss: 198.302017212\n",
      "-> current loss: 190.697433472\n",
      "-> current loss: 183.946746826\n",
      "-> current loss: 177.947006226\n",
      "-> current loss: 172.607788086\n",
      "-> current loss: 167.849578857\n",
      "-> current loss: 163.602554321\n",
      "-> current loss: 159.80531311\n",
      "-> current loss: 156.403930664\n",
      "-> current loss: 153.350982666\n",
      "-> current loss: 150.604797363\n",
      "-> current loss: 148.128799438\n",
      "-> current loss: 145.890777588\n",
      "-> current loss: 143.862442017\n",
      "-> current loss: 142.018981934\n",
      "-> current loss: 140.338500977\n",
      "-> current loss: 138.801818848\n",
      "-> current loss: 137.392044067\n",
      "-> current loss: 136.094345093\n",
      "-> current loss: 134.895690918\n",
      "-> current loss: 133.784606934\n",
      "-> current loss: 132.750991821\n",
      "-> current loss: 131.786010742\n",
      "-> current loss: 130.881835938\n",
      "-> current loss: 130.031661987\n",
      "-> current loss: 129.229446411\n",
      "-> current loss: 128.46987915\n",
      "-> current loss: 127.748291016\n",
      "-> current loss: 127.060585022\n",
      "-> current loss: 126.403167725\n",
      "-> current loss: 125.772850037\n",
      "-> current loss: 125.1668396\n",
      "-> current loss: 124.582672119\n",
      "-> current loss: 124.018180847\n",
      "-> current loss: 123.471458435\n",
      "-> current loss: 122.940826416\n",
      "-> current loss: 122.424797058\n",
      "-> current loss: 121.922073364\n",
      "-> current loss: 121.431503296\n",
      "-> current loss: 120.952041626\n",
      "-> current loss: 120.482826233\n",
      "-> current loss: 120.023033142\n",
      "-> current loss: 119.571990967\n",
      "-> current loss: 119.129058838\n",
      "-> current loss: 118.693702698\n",
      "-> current loss: 118.265411377\n",
      "-> current loss: 117.843772888\n",
      "-> current loss: 117.428405762\n",
      "-> current loss: 117.018959045\n",
      "-> current loss: 116.615135193\n",
      "-> current loss: 116.216659546\n",
      "-> current loss: 115.823287964\n",
      "-> current loss: 115.434814453\n",
      "-> current loss: 115.05103302\n",
      "-> current loss: 114.671783447\n",
      "-> current loss: 114.296897888\n",
      "-> current loss: 113.926246643\n",
      "-> current loss: 113.559692383\n",
      "-> current loss: 113.197113037\n",
      "-> current loss: 112.838424683\n",
      "-> current loss: 112.483505249\n",
      "-> current loss: 112.132286072\n",
      "-> current loss: 111.784683228\n",
      "-> current loss: 111.440620422\n",
      "-> current loss: 111.100028992\n",
      "-> current loss: 110.762840271\n",
      "-> current loss: 110.428993225\n",
      "-> current loss: 110.098442078\n",
      "-> current loss: 109.771125793\n",
      "-> current loss: 109.447006226\n",
      "-> current loss: 109.126029968\n",
      "-> current loss: 108.808151245\n",
      "-> current loss: 108.493339539\n",
      "-> current loss: 108.181541443\n",
      "-> current loss: 107.87272644\n",
      "-> current loss: 107.566856384\n",
      "-> current loss: 107.263885498\n",
      "-> current loss: 106.963798523\n",
      "-> current loss: 106.666549683\n",
      "-> current loss: 106.372116089\n",
      "-> current loss: 106.080459595\n",
      "-> current loss: 105.791549683\n",
      "-> current loss: 105.505363464\n",
      "-> current loss: 105.221870422\n",
      "-> current loss: 104.941040039\n",
      "-> current loss: 104.662849426\n",
      "-> current loss: 104.387275696\n",
      "-> current loss: 104.114280701\n",
      "-> current loss: 103.843833923\n",
      "-> current loss: 103.575927734\n",
      "-> current loss: 103.310531616\n",
      "-> current loss: 103.047607422\n",
      "-> current loss: 102.787155151\n",
      "-> current loss: 102.529136658\n",
      "-> current loss: 102.273521423\n",
      "-> current loss: 102.020301819\n",
      "-> current loss: 101.769447327\n",
      "-> current loss: 101.520927429\n",
      "-> current loss: 101.274734497\n",
      "-> current loss: 101.030838013\n",
      "-> current loss: 100.789222717\n",
      "-> current loss: 100.549865723\n",
      "-> current loss: 100.312728882\n",
      "-> current loss: 100.077819824\n",
      "-> current loss: 99.8450927734\n",
      "-> current loss: 99.6145324707\n",
      "-> current loss: 99.3861312866\n",
      "-> current loss: 99.159866333\n",
      "-> current loss: 98.9356918335\n",
      "-> current loss: 98.7136154175\n",
      "-> current loss: 98.4936141968\n",
      "-> current loss: 98.2756576538\n",
      "-> current loss: 98.0597381592\n",
      "-> current loss: 97.8458251953\n",
      "-> current loss: 97.6339111328\n",
      "-> current loss: 97.4239730835\n",
      "-> current loss: 97.2159881592\n",
      "-> current loss: 97.0099411011\n",
      "-> current loss: 96.8058166504\n",
      "-> current loss: 96.6035842896\n",
      "-> current loss: 96.4032516479\n",
      "-> current loss: 96.2047805786\n",
      "-> current loss: 96.0081558228\n",
      "-> current loss: 95.8133544922\n",
      "-> current loss: 95.6203765869\n",
      "-> current loss: 95.4291915894\n",
      "-> current loss: 95.2397918701\n",
      "-> current loss: 95.052154541\n",
      "-> current loss: 94.8662643433\n",
      "-> current loss: 94.6821060181\n",
      "-> current loss: 94.4996566772\n",
      "-> current loss: 94.3189163208\n",
      "-> current loss: 94.1398544312\n",
      "-> current loss: 93.9624557495\n",
      "-> current loss: 93.7867126465\n",
      "-> current loss: 93.6126022339\n",
      "-> current loss: 93.4401168823\n",
      "-> current loss: 93.2692337036\n",
      "-> current loss: 93.0999450684\n",
      "-> current loss: 92.9322280884\n",
      "-> current loss: 92.7660751343\n",
      "-> current loss: 92.6014633179\n",
      "-> current loss: 92.4383773804\n",
      "-> current loss: 92.2768173218\n",
      "-> current loss: 92.1167602539\n",
      "-> current loss: 91.9581985474\n",
      "-> current loss: 91.8010940552\n",
      "-> current loss: 91.6454620361\n",
      "-> current loss: 91.4912872314\n",
      "-> current loss: 91.3385314941\n",
      "-> current loss: 91.1872024536\n",
      "-> current loss: 91.0372772217\n",
      "-> current loss: 90.8887481689\n",
      "-> current loss: 90.7416000366\n",
      "-> current loss: 90.5958251953\n",
      "-> current loss: 90.4513931274\n",
      "-> current loss: 90.3083190918\n",
      "-> current loss: 90.1665649414\n",
      "-> current loss: 90.0261306763\n",
      "-> current loss: 89.8870010376\n",
      "-> current loss: 89.749168396\n",
      "-> current loss: 89.6126098633\n",
      "-> current loss: 89.4773178101\n",
      "-> current loss: 89.3432922363\n",
      "-> current loss: 89.2105102539\n",
      "-> current loss: 89.078956604\n",
      "-> current loss: 88.9486312866\n",
      "-> current loss: 88.8195114136\n",
      "-> current loss: 88.6915893555\n",
      "-> current loss: 88.5648651123\n",
      "-> current loss: 88.4393081665\n",
      "-> current loss: 88.3149185181\n",
      "-> current loss: 88.1916809082\n",
      "-> current loss: 88.0696029663\n",
      "-> current loss: 87.9486465454\n",
      "-> current loss: 87.8288116455\n",
      "-> current loss: 87.7100906372\n",
      "-> current loss: 87.5924758911\n",
      "-> current loss: 87.4759521484\n",
      "-> current loss: 87.3605041504\n",
      "-> current loss: 87.246131897\n",
      "-> current loss: 87.1328201294\n",
      "-> current loss: 87.0205612183\n",
      "-> current loss: 86.9093399048\n",
      "-> current loss: 86.7991485596\n",
      "-> current loss: 86.6899871826\n",
      "-> current loss: 86.5818328857\n",
      "-> current loss: 86.4746780396\n",
      "-> current loss: 86.368522644\n",
      "-> current loss: 86.2633590698\n",
      "-> current loss: 86.1591491699\n",
      "-> current loss: 86.0559234619\n",
      "-> current loss: 85.9536514282\n",
      "-> current loss: 85.8523178101\n",
      "-> current loss: 85.7519378662\n",
      "-> current loss: 85.6524734497\n",
      "-> current loss: 85.5539398193\n",
      "-> current loss: 85.4563217163\n",
      "-> current loss: 85.3595962524\n",
      "-> current loss: 85.2637786865\n",
      "-> current loss: 85.1688461304\n",
      "-> current loss: 85.0747909546\n",
      "-> current loss: 84.9816131592\n",
      "-> current loss: 84.889289856\n",
      "-> current loss: 84.7978210449\n",
      "-> current loss: 84.7071990967\n",
      "-> current loss: 84.6174163818\n",
      "-> current loss: 84.5284729004\n",
      "-> current loss: 84.4403457642\n",
      "-> current loss: 84.3530349731\n",
      "-> current loss: 84.2665252686\n",
      "-> current loss: 84.1808319092\n",
      "-> current loss: 84.095916748\n",
      "-> current loss: 84.0117950439\n",
      "-> current loss: 83.9284439087\n",
      "-> current loss: 83.8458633423\n",
      "-> current loss: 83.7640609741\n",
      "-> current loss: 83.6829986572\n",
      "-> current loss: 83.6026916504\n",
      "-> current loss: 83.5231323242\n",
      "-> current loss: 83.4442977905\n",
      "-> current loss: 83.3661956787\n",
      "-> current loss: 83.2888259888\n",
      "-> current loss: 83.2121582031\n",
      "-> current loss: 83.1361999512\n",
      "-> current loss: 83.0609512329\n",
      "-> current loss: 82.9863815308\n",
      "-> current loss: 82.9125137329\n",
      "-> current loss: 82.8393249512\n",
      "-> current loss: 82.7668151855\n",
      "-> current loss: 82.6949615479\n",
      "-> current loss: 82.6237869263\n",
      "-> current loss: 82.5532608032\n",
      "-> current loss: 82.4833831787\n",
      "-> current loss: 82.4141616821\n",
      "-> current loss: 82.3455734253\n",
      "-> current loss: 82.2776107788\n",
      "-> current loss: 82.2102813721\n",
      "-> current loss: 82.1435775757\n",
      "-> current loss: 82.0774765015\n",
      "-> current loss: 82.0119857788\n",
      "-> current loss: 81.9471130371\n",
      "-> current loss: 81.8828277588\n",
      "-> current loss: 81.8191299438\n",
      "-> current loss: 81.7560272217\n",
      "-> current loss: 81.6935043335\n",
      "-> current loss: 81.6315536499\n",
      "-> current loss: 81.5701828003\n",
      "-> current loss: 81.5093612671\n",
      "-> current loss: 81.4491119385\n",
      "-> current loss: 81.3894119263\n",
      "-> current loss: 81.3302612305\n",
      "-> current loss: 81.2716598511\n",
      "-> current loss: 81.2135925293\n",
      "-> current loss: 81.1560668945\n",
      "-> current loss: 81.0990600586\n",
      "-> current loss: 81.0425796509\n",
      "-> current loss: 80.986618042\n",
      "-> current loss: 80.9311752319\n",
      "-> current loss: 80.8762435913\n",
      "-> current loss: 80.8218078613\n",
      "-> current loss: 80.7678756714\n",
      "-> current loss: 80.7144470215\n",
      "-> current loss: 80.6614990234\n",
      "-> current loss: 80.6090393066\n",
      "-> current loss: 80.5570678711\n",
      "-> current loss: 80.505569458\n",
      "-> current loss: 80.454536438\n",
      "-> current loss: 80.4039764404\n",
      "-> current loss: 80.3538818359\n",
      "-> current loss: 80.3042373657\n",
      "-> current loss: 80.255065918\n",
      "-> current loss: 80.2063293457\n",
      "-> current loss: 80.1580505371\n",
      "-> current loss: 80.110206604\n",
      "-> current loss: 80.0628051758\n",
      "-> current loss: 80.015838623\n",
      "-> current loss: 79.9692993164\n",
      "-> current loss: 79.9231872559\n",
      "-> current loss: 79.8775024414\n",
      "-> current loss: 79.8322296143\n",
      "-> current loss: 79.7873687744\n",
      "-> current loss: 79.7429275513\n",
      "-> current loss: 79.6988830566\n",
      "-> current loss: 79.6552429199\n",
      "-> current loss: 79.6120071411\n",
      "-> current loss: 79.5691680908\n",
      "-> current loss: 79.5267181396\n",
      "-> current loss: 79.4846496582\n",
      "-> current loss: 79.4429702759\n",
      "-> current loss: 79.4016723633\n",
      "-> current loss: 79.3607559204\n",
      "-> current loss: 79.3202133179\n",
      "-> current loss: 79.2800292969\n",
      "-> current loss: 79.2402191162\n",
      "-> current loss: 79.2007751465\n",
      "-> current loss: 79.1616821289\n",
      "-> current loss: 79.1229553223\n",
      "-> current loss: 79.0845794678\n",
      "-> current loss: 79.0465545654\n",
      "-> current loss: 79.0088653564\n",
      "-> current loss: 78.9715270996\n",
      "-> current loss: 78.9345245361\n",
      "-> current loss: 78.8978652954\n",
      "-> current loss: 78.8615341187\n",
      "-> current loss: 78.8255386353\n",
      "-> current loss: 78.7898635864\n",
      "-> current loss: 78.754524231\n",
      "-> current loss: 78.7194976807\n",
      "-> current loss: 78.6847991943\n",
      "-> current loss: 78.6504058838\n",
      "-> current loss: 78.6163253784\n",
      "-> current loss: 78.5825576782\n",
      "-> current loss: 78.5490951538\n",
      "-> current loss: 78.5159378052\n",
      "-> current loss: 78.4830780029\n",
      "-> current loss: 78.4505233765\n",
      "-> current loss: 78.4182510376\n",
      "-> current loss: 78.3862838745\n",
      "-> current loss: 78.3546066284\n",
      "-> current loss: 78.3232116699\n",
      "-> current loss: 78.2921066284\n",
      "-> current loss: 78.2612762451\n",
      "-> current loss: 78.2307281494\n",
      "-> current loss: 78.2004547119\n",
      "-> current loss: 78.1704559326\n",
      "-> current loss: 78.1407241821\n",
      "-> current loss: 78.1112747192\n",
      "-> current loss: 78.0820770264\n",
      "-> current loss: 78.0531539917\n",
      "-> current loss: 78.0244903564\n",
      "-> current loss: 77.9960784912\n",
      "-> current loss: 77.9679260254\n",
      "-> current loss: 77.940032959\n",
      "-> current loss: 77.9123840332\n",
      "-> current loss: 77.8849868774\n",
      "-> current loss: 77.8578414917\n",
      "-> current loss: 77.8309326172\n",
      "-> current loss: 77.8042678833\n",
      "-> current loss: 77.77784729\n",
      "-> current loss: 77.7516708374\n",
      "-> current loss: 77.7257232666\n",
      "-> current loss: 77.6999969482\n",
      "-> current loss: 77.6745147705\n",
      "-> current loss: 77.6492614746\n",
      "-> current loss: 77.6242370605\n",
      "-> current loss: 77.5994415283\n",
      "-> current loss: 77.5748519897\n",
      "-> current loss: 77.5504989624\n",
      "-> current loss: 77.5263519287\n",
      "-> current loss: 77.5024337769\n",
      "-> current loss: 77.4787216187\n",
      "-> current loss: 77.4552307129\n",
      "-> current loss: 77.4319381714\n",
      "-> current loss: 77.4088668823\n",
      "-> current loss: 77.3859939575\n",
      "-> current loss: 77.3633270264\n",
      "-> current loss: 77.3408660889\n",
      "-> current loss: 77.3185958862\n",
      "-> current loss: 77.2965316772\n",
      "-> current loss: 77.2746734619\n",
      "-> current loss: 77.2530059814\n",
      "-> current loss: 77.2315216064\n",
      "-> current loss: 77.2102355957\n",
      "-> current loss: 77.1891403198\n",
      "-> current loss: 77.1682281494\n",
      "-> current loss: 77.1475067139\n",
      "-> current loss: 77.1269683838\n",
      "-> current loss: 77.1066131592\n",
      "-> current loss: 77.08644104\n",
      "-> current loss: 77.066444397\n",
      "-> current loss: 77.0466384888\n",
      "-> current loss: 77.0269851685\n",
      "-> current loss: 77.0075302124\n",
      "-> current loss: 76.9882278442\n",
      "-> current loss: 76.9691162109\n",
      "-> current loss: 76.9501571655\n",
      "-> current loss: 76.9313812256\n",
      "-> current loss: 76.9127578735\n",
      "-> current loss: 76.8943099976\n",
      "-> current loss: 76.8760147095\n",
      "-> current loss: 76.8578872681\n",
      "-> current loss: 76.8399200439\n",
      "-> current loss: 76.8221130371\n",
      "-> current loss: 76.8044662476\n",
      "-> current loss: 76.7869644165\n",
      "-> current loss: 76.7696228027\n",
      "-> current loss: 76.7524337769\n",
      "-> current loss: 76.7354049683\n",
      "-> current loss: 76.7185211182\n",
      "-> current loss: 76.7017822266\n",
      "-> current loss: 76.6851806641\n",
      "-> current loss: 76.6687393188\n",
      "-> current loss: 76.6524429321\n",
      "-> current loss: 76.6362838745\n",
      "-> current loss: 76.6202697754\n",
      "-> current loss: 76.6043930054\n",
      "-> current loss: 76.5886611938\n",
      "-> current loss: 76.573059082\n",
      "-> current loss: 76.5576019287\n",
      "-> current loss: 76.5422821045\n",
      "-> current loss: 76.52709198\n",
      "-> current loss: 76.5120239258\n",
      "-> current loss: 76.4971084595\n",
      "-> current loss: 76.4823074341\n",
      "-> current loss: 76.4676361084\n",
      "-> current loss: 76.4531021118\n",
      "-> current loss: 76.4386901855\n",
      "-> current loss: 76.4244003296\n",
      "-> current loss: 76.4102401733\n",
      "-> current loss: 76.3962020874\n",
      "-> current loss: 76.3822784424\n",
      "-> current loss: 76.3684844971\n",
      "-> current loss: 76.3548126221\n",
      "-> current loss: 76.341255188\n",
      "-> current loss: 76.3278198242\n",
      "-> current loss: 76.314491272\n",
      "-> current loss: 76.30128479\n",
      "-> current loss: 76.288192749\n",
      "-> current loss: 76.2752151489\n",
      "-> current loss: 76.262336731\n",
      "-> current loss: 76.2495803833\n",
      "-> current loss: 76.2369384766\n",
      "-> current loss: 76.2244033813\n",
      "-> current loss: 76.2119750977\n",
      "-> current loss: 76.1996459961\n",
      "-> current loss: 76.1874313354\n",
      "-> current loss: 76.1753158569\n",
      "-> current loss: 76.1633071899\n",
      "-> current loss: 76.1514053345\n",
      "-> current loss: 76.1395950317\n",
      "-> current loss: 76.1278915405\n",
      "-> current loss: 76.1163024902\n",
      "-> current loss: 76.1047973633\n",
      "-> current loss: 76.0933914185\n",
      "-> current loss: 76.0820846558\n",
      "-> current loss: 76.0708770752\n",
      "-> current loss: 76.0597686768\n",
      "-> current loss: 76.0487442017\n",
      "-> current loss: 76.0378189087\n",
      "-> current loss: 76.0269775391\n",
      "-> current loss: 76.016242981\n",
      "-> current loss: 76.0055999756\n",
      "-> current loss: 75.9950332642\n",
      "-> current loss: 75.9845657349\n",
      "-> current loss: 75.9741821289\n",
      "-> current loss: 75.9638977051\n",
      "-> current loss: 75.9536895752\n",
      "-> current loss: 75.9435653687\n",
      "-> current loss: 75.9335403442\n",
      "-> current loss: 75.9235916138\n",
      "-> current loss: 75.9137115479\n",
      "-> current loss: 75.9039382935\n",
      "-> current loss: 75.894241333\n",
      "-> current loss: 75.8846206665\n",
      "-> current loss: 75.8750762939\n",
      "-> current loss: 75.8656311035\n",
      "-> current loss: 75.8562469482\n",
      "-> current loss: 75.8469467163\n",
      "-> current loss: 75.8377227783\n",
      "-> current loss: 75.8285827637\n",
      "-> current loss: 75.8195114136\n",
      "-> current loss: 75.8105163574\n",
      "-> current loss: 75.8015975952\n",
      "-> current loss: 75.792755127\n",
      "-> current loss: 75.7839813232\n",
      "-> current loss: 75.7752838135\n",
      "-> current loss: 75.7666625977\n",
      "-> current loss: 75.758102417\n",
      "-> current loss: 75.7496185303\n",
      "-> current loss: 75.7412033081\n",
      "-> current loss: 75.7328720093\n",
      "-> current loss: 75.7245941162\n",
      "-> current loss: 75.7163848877\n",
      "-> current loss: 75.7082443237\n",
      "-> current loss: 75.7001724243\n",
      "-> current loss: 75.6921691895\n",
      "-> current loss: 75.6842269897\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for e in range(nb_epoch):\n",
    "    c = train_model(X, y)\n",
    "    print('-> current loss:', c)\n",
    "    losses.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14bd1b450>]"
      ]
     },
     "execution_count": 1331,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12e019ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "First, we need to re-initialize our shared variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1338,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rng.seed(156651)\n",
    "weights = np.asarray(rng.uniform(low=-0.05, high=0.05,\n",
    "                                 size=(X.shape[1])),\n",
    "                     dtype='float32')\n",
    "W.set_value(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start looping and each time we update our parameters in the light of their respective gradients using a simple update rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1339,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current loss -> 1146.99194336\n",
      "current loss -> 127.060585022\n",
      "current loss -> 106.666549683\n",
      "current loss -> 94.8662643433\n",
      "current loss -> 87.4759521484\n",
      "current loss -> 82.8393249512\n",
      "current loss -> 79.9231872559\n",
      "current loss -> 78.0820770264\n",
      "current loss -> 76.9127578735\n",
      "current loss -> 76.1633071899\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for e in range(nb_epoch):\n",
    "    predictions = predict(X)\n",
    "    curr_loss = mse(predictions, y)\n",
    "    if e % 50 == 0:\n",
    "        print('current loss ->', curr_loss)\n",
    "    losses.append(curr_loss)\n",
    "    curr_grads = get_grads(X, y)\n",
    "    curr_weights = W.get_value(borrow=True)\n",
    "    curr_weights = curr_weights - learning_rate * curr_grads\n",
    "    W.set_value(curr_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we obtain a loss curve that looks very similar to our previous one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12df9fe90>]"
      ]
     },
     "execution_count": 1340,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14bdf6310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Perceptron in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(For reference, I include a similar implementation of our perceptron in `Tensorflow`, but we probably won't have tim eto work through block by block...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1382,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1383,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "X = np.array(X, dtype='float32')\n",
    "y = np.array(y, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1384,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# purely \"symbolic variable\"\n",
    "train_input = tf.placeholder(tf.float32, [None, X.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1385,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_output = tf.placeholder(tf.float32, [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1386,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rng.seed(156651)\n",
    "weights = tf.Variable(np.asarray(rng.uniform(low=-0.05, high=0.05,\n",
    "                                 size=(X.shape[1])),\n",
    "                     dtype='float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1387,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_output = tf.reduce_sum(tf.mul(train_input, weights), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1388,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mse_cost = tf.reduce_mean(tf.square(model_output - train_output),\n",
    "                          reduction_indices=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1389,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.0000001).minimize(mse_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1390,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1391,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1392,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1146.99\n",
      "1146.99\n"
     ]
    }
   ],
   "source": [
    "f = sess.run(model_output, feed_dict={train_input: X})\n",
    "print(mean_squared_error(f, y))\n",
    "g = sess.run(mse_cost, feed_dict={train_input: X, train_output: y})\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1393,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1146.99\n",
      "127.061\n",
      "106.667\n",
      "94.8663\n",
      "87.476\n",
      "82.8393\n",
      "79.9232\n",
      "78.0821\n",
      "76.9128\n",
      "76.1633\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for i in range(500):\n",
    "    c = sess.run(mse_cost, feed_dict={train_input: X, train_output: y})\n",
    "    losses.append(c)\n",
    "    if i % 50 == 0:\n",
    "        print(c)\n",
    "    sess.run(train_step, feed_dict={train_input: X, train_output: y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1394,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15dcedc90>]"
      ]
     },
     "execution_count": 1394,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15aff5fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
